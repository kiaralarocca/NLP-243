# -*- coding: utf-8 -*-
"""run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-6E475tEWavAVakNKh1S9GkUDzTX3dao
"""

#!pip install scikit-learn matplotlib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader


#from google.colab import drive : local loading
#drive.mount('/content/drive')
# Path to your file in Google Drive
#file_path = '/content/drive/My Drive/hw1_train.csv'
# Read the CSV file
#df = pd.read_csv(file_path)

# Check the first few rows
#print(df.head())

if __name__ == '__main__':
    if len(sys.argv) != 4:
        print("Usage: python run.py <train_file> <test_file> <output_file>")
        sys.exit(1)

    train_file = sys.argv[1]
    test_file = sys.argv[2]
    output_file = sys.argv[3]

# Read the training CSV file
df = pd.read_csv(train_file)
print(f"Training data shape: {df.shape}")
print(df.head())  # Display the first few rows

# Read the test CSV file
df_test = pd.read_csv(test_file)
print(f"Test data shape: {df_test.shape}")
print(df_test.head())  # Display the first few rows

# Step 3: Split the data into features (X) and labels (y)
X = df['UTTERANCES'].values
y = df['CORE RELATIONS'].values

# Step 4: Vectorize the 'UTTERANCES' using CountVectorizer (Bag of Words approach)
vectorizer = CountVectorizer()
X_vectorized = vectorizer.fit_transform(X)

# Step 5: Encode the 'CORE RELATIONS' labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Step 6: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_encoded, test_size=0.2, random_state=42)

# Output: Display the shape of the training and testing datasets, and the number of unique labels
print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")
print(f"Number of unique classes: {len(label_encoder.classes_)}")

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Create TensorDataset for training and test data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# Define DataLoader for batching
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) #32 -> 10
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Second hidden layer
        self.fc3 = nn.Linear(hidden_size, num_classes)  # Output layer

    def forward(self, x):
        x = F.leaky_relu(self.fc1(x))  # Apply ReLU activation to first layer
        x = self.dropout(x)
        x = F.leaky_relu(self.fc2(x))  # Apply ReLU activation to second layer
        x = self.fc3(x)  # Output layer without activation (logits)
        return x

# Initialize the model
input_size = X_train_tensor.shape[1]  # 1132 features
hidden_size = 256  # You can adjust this: 128 ->256
num_classes = len(label_encoder.classes_)  # 47 classes

model = MLP(input_size, hidden_size, num_classes)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Using Adam optimizer

# Train the model
num_epochs = 10  # You can adjust this
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')
            running_loss = 0.0

# Evaluate the model on the test data
model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
y_true = []
y_pred = []

with torch.no_grad():  # Disable gradient calculation
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability
        y_true.extend(labels.numpy())
        y_pred.extend(predicted.numpy())
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

#confusion matrix
cm = confusion_matrix(y_true, y_pred)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()


print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')
f1 = f1_score(labels, predicted, average='macro')
print(f'F1 Score: {f1:.4f}')

# Save the model's state dictionary (recommended approach)
torch.save(model.state_dict(), 'mlp_model.pth')

# Loaded df_test earlier

# Vectorize the 'UTTERANCES' using the same vectorizer
new_X_vectorized = vectorizer.transform(df_test['UTTERANCES'])

# Convert the vectorized data to PyTorch tensor
new_X_tensor = torch.tensor(new_X_vectorized.toarray(), dtype=torch.float32)

# Set the model to evaluation mode
model.eval()

# Make predictions
with torch.no_grad():  # Disable gradient calculation
    outputs = model(new_X_tensor)
    _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability

# Convert predictions back to original labels
predicted_labels = label_encoder.inverse_transform(predicted.numpy())

# Step 6: Create a DataFrame for the submission
submission_df = pd.DataFrame({
    'ID': df_test['ID'],  # Replace with your actual ID column name
    'Core Relations': predicted_labels
})

# Step 7: Save the submission DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)

print("Predicted results saved to 'submission.csv'.")

# Optionally, you can add the predictions back to the DataFrame
df_test['Core Relations'] = predicted_labels

# Display the results
print(df_test[['UTTERANCES', 'Core Relations']])

# Get the class labels sorted in alphabetical order
sorted_labels = sorted(label_encoder.classes_)
print("Class labels sorted in alphabetical order:", sorted_labels)

# Count the occurrences of each class in the 'CORE RELATIONS' column
class_counts = df_test['Core Relations'].value_counts()

# Create a bar plot for class distribution
plt.figure(figsize=(10, 6))
class_counts.plot(kind='bar', color='skyblue')
plt.title('Class Distribution of Core Relations')
plt.xlabel('Core Relations')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y')

# Show the plot
plt.tight_layout()  # Adjust layout for better visibility
plt.show()

